{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smnd/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "Epoch = 1, Batch = 214, loss = 0.15921, accuracy = 0.96094\n",
      "Epoch = 2, Batch = 214, loss = 0.04772, accuracy = 0.97266\n",
      "Epoch = 3, Batch = 214, loss = 0.04263, accuracy = 0.97266\n",
      "Epoch = 4, Batch = 214, loss = 0.04920, accuracy = 0.98047\n",
      "Epoch = 5, Batch = 214, loss = 0.06713, accuracy = 0.97656\n",
      "Epoch = 6, Batch = 214, loss = 0.04228, accuracy = 0.97656\n",
      "Epoch = 7, Batch = 214, loss = 0.01150, accuracy = 0.97656\n",
      "Epoch = 8, Batch = 214, loss = 0.00789, accuracy = 0.98438\n",
      "Epoch = 9, Batch = 214, loss = 0.01954, accuracy = 0.98438\n",
      "Epoch = 10, Batch = 214, loss = 0.01903, accuracy = 0.98438\n",
      "fuck\n"
     ]
    }
   ],
   "source": [
    "# My own Recurrent Neural Network(LSTM)\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#importing MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters ------------------------------------------------------------------------------\n",
    "\n",
    "HLS = 256               # Hidden Layer Size\n",
    "num_layers = 1          # Stacked Number of LSTM\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 256       # data / 1 batch\n",
    "n_seq = 28\n",
    "\n",
    "dropout = 0.5          # dropout\n",
    "n_classes = 10\n",
    "\n",
    "valid_data_size = 256\n",
    "\n",
    "\n",
    "# ======================================================================================= #\n",
    "\n",
    "\n",
    "def input_data(arr, n_seqs=1, n_steps=28):\n",
    "    # in 1 image\n",
    "    #small_batch_length = n_seqs * n_steps\n",
    "    \n",
    "    #for i in range(arr.shape[0]):\n",
    "    #    arr[i,:,:,:] = arr[i,:small_batch_length,:,:]\n",
    "    \n",
    "    return 0 # randomly smapling method\n",
    "\n",
    "def reshape_input(x, n_seq, n_data):\n",
    "    X_in = tf.reshape(x, [-1,n_seq,n_data])\n",
    "    return X_in\n",
    "\n",
    "\n",
    "def fully_connected(input_layer, weights, biases, dropout):\n",
    "    \n",
    "    input_layer = tf.reshape(input_layer, [-1, weights['w1'].get_shape().as_list()[0]] ) # check\n",
    "    \n",
    "    \n",
    "    hl1 = tf.add(tf.matmul(input_layer, weights['w1']), biases['b1'])\n",
    "    hl1 = tf.nn.relu(hl1)\n",
    "    hl1 = tf.nn.dropout(hl1, dropout)\n",
    "    \n",
    "    out = tf.add(tf.matmul(hl1, weights['wout']), biases['bout'])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def build_lstm(hidden_size, num_layers, batch_size, keep_prob):\n",
    "    '''\n",
    "    lstm_size : Size of Hidden Layers in the LSTM celber of LSTM Layers.ls\n",
    "    num_layers : Number of LSTM Layers\n",
    "    '''\n",
    "    lstm = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    #tf.contrib.BasicLSTMCell(hidden_size)\n",
    "    \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "    \n",
    "    \n",
    "    # Stacking up\n",
    "    cell = tf.contrib.rnn.MultiRNNCell( [drop]*num_layers )\n",
    "    initial_state = cell.zero_state( batch_size, tf.float32 )\n",
    "    \n",
    "    return cell, initial_state\n",
    "\n",
    "# ======================================================================================= #\n",
    "\n",
    "\n",
    "# Weights & Biases ---------------------------------------------------------------------------\n",
    "\n",
    "# -------------------- def Xavier_initializer -------------------- #\n",
    "def Xavier_init(name, shape_in):\n",
    "    return tf.get_variable(name, shape=shape_in, initializer=tf.contrib.layers.xavier_initializer())\n",
    "# ---------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'w1' : Xavier_init('w1',[n_seq*HLS, 1024]),\n",
    "    'wout': Xavier_init('wout',[1024, n_classes])\n",
    "}\n",
    "\n",
    "biases = {\n",
    "\n",
    "    'b1' : Xavier_init('b1',[1024]),\n",
    "    'bout': Xavier_init('bout',[n_classes])\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input & output placeholder -------------------------------------------------------------------\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob_dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model ----------------------------------------------------------------------------------------\n",
    "\n",
    "rnn_cell, init_state = build_lstm(HLS, num_layers, batch_size, keep_prob_dropout) # works only with hyperparameter\n",
    "\n",
    "X_in = reshape_input(x, n_seq=28, n_data=28)\n",
    "\n",
    "output_data, state = tf.nn.dynamic_rnn(rnn_cell, X_in, initial_state=init_state, dtype=tf.float32) \n",
    "                                            # input x : batch_size, max_time, depth\n",
    "\n",
    "logits = fully_connected(output_data, weights, biases, keep_prob_dropout)\n",
    "\n",
    "# loss and optimizer ---------------------------------------------------------------------------\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy -------------------------------------------------------------------------------------\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1)) # 1???? test later\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver ----------------------------------------------------------------------------------------\n",
    "save_file = './SaveDataRNN/test1.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Training start -------------------------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={x:batch_x, y:batch_y, keep_prob_dropout: dropout}) # ...? drop out?\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob_dropout: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:valid_data_size],\n",
    "                y: mnist.validation.labels[:valid_data_size],\n",
    "                keep_prob_dropout: 1.})\n",
    "\n",
    "            # print('Epoch = {:}, Batch = {:}, loss = {:.5f}, accuracy = {:.5f}'.format(epoch+1, batch+1, loss, valid_acc))\n",
    "            # saver.save(sess, save_file)\n",
    "        \n",
    "        print('Epoch = {:}, Batch = {:}, loss = {:.5f}, accuracy = {:.5f}'.format(epoch+1, batch+1, loss, valid_acc))\n",
    "        saver.save(sess, save_file)\n",
    "    print('fuck')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "55000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a = tf.constant(2, tf.float32)\n",
    "b = tf.random_normal([3,2,2])\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "a = mnist.train.num_examples\n",
    "print(mnist.train.num_examples)\n",
    "\n",
    "np.shape(mnist.validation.images[:200])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
